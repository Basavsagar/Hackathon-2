{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d676435e908abb9226cd620d6d8bece11f2daf8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42f6edda3b24234d471c2003a94e80c2baee8ff2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "720987f663f04c2a23badf83acff42f5ac2122d4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I import the libraries as and when they are required in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "dadbbf240f10959a7c3956eeb7b04c3f292bf37d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'indian_liver_patient.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1642aea3ff14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'indian_liver_patient.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mip_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#See the types of data and missing values in the dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, dtype, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     return io._parse_excel(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# a ZIP file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'indian_liver_patient.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "raw_data = pd.read_excel('indian_liver_patient.csv')\n",
    "ip_data = raw_data.copy()\n",
    "\n",
    "#See the types of data and missing values in the dataset.\n",
    "# Categorical data (like gender) need to be converted using dummy variables. \n",
    "ip_data.info()\n",
    "\n",
    "#ip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b8b21b27a1920dcbe076a4673b380e890f4b1cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See the types of data and missing values in the dataset.\n",
    "# Categorical data (like gender) need to be converted using dummy variables. \n",
    "ip_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32ae92561113a6ec184861aef25a1b1a28ae11b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since there are only 4 records with null values in Albumin_and_Globulin_Ratio column, we can drop those records.\n",
    "ip_data = ip_data.dropna(how='any', axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bffaf433b8596e5abea2e3df61dc76e1c85683ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert Gender to 0s (for Male) and 1s (for Female)\n",
    "ip_data['Gender'] = ip_data['Gender'].map({'Male':0, 'Female':1})\n",
    "ip_data['Dataset'] = ip_data['Dataset'].map({2:0, 1:1}) # To solve ValueError: endog must be in the unit interval.\n",
    "\n",
    "# Check if mapping happened properly\n",
    "ip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2c388a2eba0243801c796a0f1ff01db8975d83b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correlations between variables help us identify the features that can be excluded. \n",
    "# We can exclude one in two features which has a strong correlatoin (|correlation| > 0.5) with another feature\n",
    "# e.g: In the below given case, Total_Bilirubin and Direct_Bilirubin are strongly correlated. So we can\n",
    "# discard one of those features. The exclusion of features happens 2 cells down.  \n",
    "ip_corr = ip_data.drop(['Gender', 'Dataset'], axis = 1)\n",
    "ip_corr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c30d6eeb1db4fb9cc5fb306126fe6810ea47c64"
   },
   "source": [
    "### Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "367a550137a23a50dd628f2bee28fd872b3def7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_count = ip_data.shape[0]\n",
    "# You will see drastic change in models' accuracy when you change the train and test sample proportion (80:20, 70:30 etc)\n",
    "ip_train_count = int(0.8*samples_count) \n",
    "ip_test_count = samples_count - ip_train_count\n",
    "\n",
    "ip_train_data = ip_data[:ip_train_count]\n",
    "ip_test_data = ip_data[ip_train_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5a58919205d48de3ec0ed28897d2058870a6301",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ip_train_count)\n",
    "print(ip_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d17b5912ee18ea69fce18fd3faaa447fa557145"
   },
   "source": [
    "### Declare the Dependent (y_...) and Independent (X_...) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99b7dce0bd1a141639284375ff868c86e0dc1bfd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35e756c0bc1ac131d3d5f7ded379d12285c4a7b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## You can start with all features and then optimize the model by removing the features from the model.\n",
    "\n",
    "X1_train = ip_train_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n",
    "              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n",
    "\n",
    "#X1_train = ip_train_data[['Age', 'Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin']]\n",
    "\n",
    "X_train= sm.add_constant(X1_train)\n",
    "#X_train= X1_train.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\n",
    "y_train = ip_train_data['Dataset']\n",
    "\n",
    "\n",
    "X1_test = ip_test_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n",
    "              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n",
    "\n",
    "#X1_test = ip_test_data[['Age','Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin' ]]\n",
    "\n",
    "X_test = sm.add_constant(X1_test)\n",
    "#X_test = X1_test.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\n",
    "y_test = ip_test_data['Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbbcb74a516997cd8885db3c5dd6eb286ec65645"
   },
   "source": [
    "## Logit function from statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f8713514177da57b34c211163cb6ded7b6add5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_log = sm.Logit(y_train,X_train)\n",
    "result_log = reg_log.fit()\n",
    "\n",
    "result_log.summary2()\n",
    "\n",
    "# See the values against each feature in 'P>|z|' column. The value less than 0.05 means the feature is insignificant in the model.\n",
    "# Surprisingly, gender is insignificant in this model. \n",
    "# We saw a strong correlation between Total_Bilirubin and Direct_Bilirubin, and we could drop one of those features.\n",
    "# But in this model both Total_Bilirubin and Direct_Bilirubin are insignificant(P > 0.05). So we must discard both features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e9a2131f2826cfa24e8eed1329f101e7c00372e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix using the train dataset\n",
    "result_log.predict()\n",
    "result_log.pred_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4df92d1c6038a22f54cff7741974e6043fe793a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print X_test and X_train and see if the order of features are same in both datasets\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da069d425756caf1751081f9273f7a7b25fa1c87",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e074d492d8896861b5fdec66baabc65ca480e41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_predicted = result_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a234ccdec1b9382916b37a2d5609ba0f6fdf5c25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below line, if you wanted to see the predicted values\n",
    "# logit_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c297f005a8b5babd19f9cb0e3478d0c8d592278",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix of predicted values. if you wanted to use sklearn's 'confusion_matrix', \n",
    "# you should convert float values in 'logit_predicted' to 0s and 1s.\n",
    "\n",
    "# Here I use a custom confusion matrix code \n",
    "bins = np.array([0,0.5,1])\n",
    "cm_log = np.histogram2d(y_test, logit_predicted, bins = bins)[0]\n",
    "logit_accuracy = (cm_log[0,0] + cm_log[1,1])/cm_log.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c07e217d1f085d1835dda360d9ad061497d5ea79",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix (Logit): \\n', cm_log)\n",
    "print('---------------------------------------------')\n",
    "print('Accuracy (Logit): \\n', round(logit_accuracy*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b1780e0f23414d9b9fee35708bf113acab54b43"
   },
   "source": [
    "## Logitstic Regression (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3b4f2e8f164f0eca5076a2398bd0bc1fd4fbf81",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "log_predicted = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2facd5fac11c1d20dd9c35eea431364e659caaf7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix (Logistic Reg): \\n', confusion_matrix(y_test,log_predicted))\n",
    "print('---------------------------------------------')\n",
    "print('Accuracy (Logistict Reg): \\n', round(accuracy_score(y_test, log_predicted)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0192e0044814301cb80e66d9d23ffa6247219283"
   },
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9857727828e23a6d0fb4502668510b6aeb0e063e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "result_gauss = gaussian.fit(X_train,y_train)\n",
    "\n",
    "gauss_predicted = gaussian.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ae3a09c35e82d01a3b921bbc21ff7afa7c74b7d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below line, if you wanted to see the predicted values\n",
    "#gauss_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99dd0219b58cd48ba13c84c3500ca22c8fb7bbe4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix (Gaussian): \\n', confusion_matrix(y_test,gauss_predicted))\n",
    "print('---------------------------------------------')\n",
    "print('Accuracy (Gaussian): \\n', round(accuracy_score(y_test, gauss_predicted)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1f3a5b391aaaf5d28447c4bb7f1eab1587c4ec3"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de12744a1800adc97131ed6160c08f39ab991484",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "rf_predicted = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "538e999531a379946a55d65a457e4e92eb1091b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix (Random Forest): \\n', confusion_matrix(y_test,rf_predicted))\n",
    "print('---------------------------------------------')\n",
    "print('Accuracy (Random Forest): \\n', round(accuracy_score(y_test, rf_predicted)*100,2), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30f6f4151b0c15ff76aacdbabb66e28e71953581",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
